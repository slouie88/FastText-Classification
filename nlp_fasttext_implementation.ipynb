{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Classification\n",
    "FastText[1] is a neural network based text classification model designed to be computationally efficient. The code below implements the basic architecture of the model, described in (2).\n",
    "\n",
    "The model will be trained using the Adam algorithm as opposed to mini-batch gradient descent, as it was found to improve accuracy significantly on the dataset to be used. The Adam algorithm is a computationally efficient first-order gradient-based optimization of stochastic gradient functions, based on adaptive estimates of lower-order moments.\n",
    "\n",
    "When the training data are sequences of variable lengths we can not simply stack multiple training sequences into one tensor. Instead, it is common to assume that there is a maximal sequence length, so that all sequences in a batch are fitted into tensors of the same dimensions. For sequences shorter than the maximal length, we append them with a special pad word so that all sequences in a batch are of the same length. A pad word is a special token, whose embedding is an all-zero vector, so that the presence of pad words does not change the output of the model. In this code, the pad word has an ID of 0. Additionally, the number of words that are in each input sentence (before they got padded to be the same length) are provided as an input parameter to the FastText model.\n",
    "\n",
    "[1] Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas. Bag of Tricks for Efficient Text Classification. arXiv preprint arXiv:1607.01759., 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from collections import namedtuple\n",
    "\n",
    "import sys, getopt\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "learning_rate = 0.005\n",
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    \"\"\"Define the computation graph for fasttext model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_classes, embedding_dim, learning_rate):\n",
    "        \"\"\"Init the model with default parameters/hyperparameters.\"\"\"\n",
    "        super(FastText, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_func = F.cross_entropy\n",
    "        # create all the variables (weights) that the model needs here\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output = nn.Linear(self.embedding_dim, self.num_classes)\n",
    "        self.embedder = nn.Embedding(self.vocab_size, self.embedding_dim, 0)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, sens_lengths):\n",
    "        \"\"\"Implement the FastText computation\"\"\"\n",
    "        embedded = self.embedder(x)\n",
    "        #print(embedded)\n",
    "        x = embedded.sum(dim=1) / sens_lengths\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences is 8216. \n",
      "PAD word id is 0 .\n",
      "Unknown word id is 1 .\n",
      "size of vocabulary is 3666. \n",
      "read 1000 sentences from question_2-1_data\\sentences_train.txt .\n",
      "read 500 sentences from question_2-1_data\\sentences_dev.txt .\n",
      "read 500 sentences from question_2-1_data\\sentences_test.txt .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FastText. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : train loss = 1.1029887515306473 , validation accuracy = 0.40799999237060547 .\n",
      "Epoch 1 : train loss = 1.0515777665376662 , validation accuracy = 0.421999990940094 .\n",
      "Epoch 2 : train loss = 0.9764573341608047 , validation accuracy = 0.4440000057220459 .\n",
      "Accuracy on the test set : 0.4560000002384186.\n"
     ]
    }
   ],
   "source": [
    "from fasttext import load_question_2_1, train_fast_text\n",
    "\n",
    "word_to_id, train_data, valid_data, test_data = load_question_2_1('question_2-1_data')\n",
    "model = FastText(len(word_to_id)+2, num_classes, embedding_dim=embedding_dim, learning_rate=learning_rate)\n",
    "\n",
    "model_file_path = os.path.join('models', 'fasttext_model_file_q2-1')\n",
    "train_fast_text(model, train_data, valid_data, test_data, model_file_path, batch_size=batch_size, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset provided contains three files: **train.json**, **validation.json**, and **test.json**, which are the training dataset, validation dataset, and the test dataset, respectively. \n",
    "See an example below: \n",
    "```\n",
    "{\n",
    "   \"ID\": S1,\n",
    "   \"Label\": 3,\n",
    "   \"Sentence\":\"What country has the best defensive position in the board game Diplomacy ?\"\n",
    "}\n",
    "```\n",
    "In the training set and the validation set, the response variable is called `Label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # You can use this library to read the .json files into a Python dict: https://docs.python.org/2/library/json.html\n",
    "from nltk import word_tokenize # You can use this to tokenize strings, or do your own pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords, brown\n",
    "from nltk import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger, pos_tag\n",
    "\n",
    "\n",
    "# Train a mixture of Trigram, Bigram and Unigram POS taggers (implements 'backoff' method)\n",
    "brown_train = brown.tagged_sents(tagset=\"universal\")\n",
    "t0 = DefaultTagger(\"NOUN\")\n",
    "t1 = UnigramTagger(brown_train, backoff=t0)\n",
    "t2 = BigramTagger(brown_train, backoff=t1)\n",
    "t3 = TrigramTagger(brown_train, backoff=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences is 221205. \n",
      "PAD word id is 0 .\n",
      "Unknown word id is 1 .\n",
      "size of vocabulary is 44914. \n",
      "Epoch 0 : train loss = 0.6694232958394128 , validation accuracy = 0.9279999732971191 .\n",
      "Epoch 1 : train loss = 0.0261098971854694 , validation accuracy = 0.9300000071525574 .\n",
      "Epoch 2 : train loss = 0.0027319554324463213 , validation accuracy = 0.9369999766349792 .\n",
      "Epoch 3 : train loss = 0.0013911327912717604 , validation accuracy = 0.9430000185966492 .\n",
      "Epoch 4 : train loss = 0.000524844769569353 , validation accuracy = 0.9369999766349792 .\n",
      "Epoch 5 : train loss = 0.00031905708247732773 , validation accuracy = 0.9430000185966492 .\n",
      "Epoch 6 : train loss = 0.0002386649818642606 , validation accuracy = 0.9430000185966492 .\n",
      "Epoch 7 : train loss = 0.0001895769731876302 , validation accuracy = 0.9409999847412109 .\n",
      "Epoch 8 : train loss = 0.00015442724277916667 , validation accuracy = 0.9430000185966492 .\n",
      "Epoch 9 : train loss = 0.00012719354492366766 , validation accuracy = 0.9430000185966492 .\n",
      "Epoch 10 : train loss = 0.0001081916961010319 , validation accuracy = 0.9430000185966492 .\n",
      "Epoch 11 : train loss = 9.29071207006141e-05 , validation accuracy = 0.9419999718666077 .\n",
      "Epoch 12 : train loss = 8.047182219908447e-05 , validation accuracy = 0.9390000104904175 .\n",
      "Epoch 13 : train loss = 7.022172689470903e-05 , validation accuracy = 0.9409999847412109 .\n",
      "Epoch 14 : train loss = 6.210421221063321e-05 , validation accuracy = 0.9390000104904175 .\n",
      "Epoch 15 : train loss = 5.526078857164484e-05 , validation accuracy = 0.9390000104904175 .\n",
      "Epoch 16 : train loss = 4.9390717494306217e-05 , validation accuracy = 0.9390000104904175 .\n",
      "Epoch 17 : train loss = 4.4490587660638504e-05 , validation accuracy = 0.9390000104904175 .\n",
      "Epoch 18 : train loss = 3.984343254922203e-05 , validation accuracy = 0.9390000104904175 .\n",
      "Epoch 19 : train loss = 3.607940915743883e-05 , validation accuracy = 0.9390000104904175 .\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "num_classes = 6\n",
    "learning_rate = 0.1\n",
    "embedding_dim = 10\n",
    "\n",
    "pad_word_id = 0\n",
    "unknown_word_id = 1\n",
    "\n",
    "from prepros import preprocessor\n",
    "from fasttext import Dataset, map_word_to_id, map_token_seq_to_word_id_seq\n",
    "\n",
    "\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    \"\"\"\n",
    "    Applies preprocessing to given string of text.\n",
    "    \n",
    "    Inputs:\n",
    "        text (string) : input string to preprocess\n",
    "        \n",
    "    Outputs:\n",
    "        tokens (list) : list of preprocessed tokens\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    tokens = word_tokenize(text)                                 # Split sentence into tokens\n",
    "    tokens = [i for i in tokens if i not in stopwords_en]        \n",
    "    \n",
    "    bigram = ngrams(tokens, 2)                                   # Extract bigrams from tokens\n",
    "    bigrams = [\"\".join(i) for i in bigram]\n",
    "    \n",
    "    tags = t3.tag(tokens)                                        # POS tag tokens\n",
    "    tags = [\"\".join(i) for i in tags]\n",
    "    \n",
    "    output.extend(bigrams)\n",
    "    output.extend(tags)\n",
    "    output.extend(tokens)\n",
    "\n",
    "    return output\n",
    "    \n",
    "    \n",
    "def build_vocabulary(training_dict):\n",
    "    \"\"\"\n",
    "    Build a vocabulary from the training data set.\n",
    "    \n",
    "    Inputs:\n",
    "        training_dict (dictionary): data from json training data\n",
    "    Outputs:\n",
    "        word_to_id (dictionary) : A dictionary mapping of words to ids\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for i in training_dict:\n",
    "        tokens = word_tokenize(i['Sentence'])          # Split sentence into tokens       \n",
    "        bigram = ngrams(tokens, 2)                     # Extract bigrams from tokens             \n",
    "        tags = t3.tag(tokens)                          # POS tag word tokens\n",
    "        \n",
    "        bigrams = [\"\".join(j) for j in bigram]\n",
    "        tags = [\"\".join(k) for k in tags]\n",
    "        data.extend(bigrams)\n",
    "        data.extend(tags)\n",
    "        data.extend(tokens)\n",
    "    print('number of sequences is %s. ' % len(data))\n",
    "    count = [['$PAD$', pad_word_id], ['$UNK$', unknown_word_id]]\n",
    "    sorted_counts = collections.Counter(data).most_common()\n",
    "    count.extend(sorted_counts)\n",
    "    word_to_id = dict()\n",
    "    for word, _ in count:\n",
    "        word_to_id[word] = len(word_to_id)\n",
    "\n",
    "    print(\"PAD word id is %s .\" % word_to_id['$PAD$'])\n",
    "    print(\"Unknown word id is %s .\" % word_to_id['$UNK$'])\n",
    "    print('size of vocabulary is %s. ' % len(word_to_id))\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def create_dataset(sentence_dict, word_to_id):\n",
    "    \"\"\"\n",
    "    Create dataset given data loaded from JSON file.\n",
    "    \n",
    "    Inputs:\n",
    "        sentence_dict (dictionary): data from JSON file\n",
    "        word_to_id (dictionary) : dictionary mapping of words to their ID's\n",
    "        \n",
    "    Outputs:\n",
    "        dataset (Dataset) : Dataset object\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in sentence_dict:\n",
    "        word_id_seq = map_token_seq_to_word_id_seq(custom_preprocessor(i['Sentence']), word_to_id)        \n",
    "        sentences.append(word_id_seq)\n",
    "        if 'Label' in i:\n",
    "            labels.append(i['Label'])\n",
    "            \n",
    "    if len(labels) == 0:\n",
    "        dataset = Dataset(sentences)\n",
    "    else:\n",
    "        dataset = Dataset(sentences, labels)\n",
    "        \n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "\n",
    "def load_question_2_2(data_folder):\n",
    "    \"\"\"\n",
    "    Prepare and load relevant files for training FastText model.\n",
    "    \n",
    "    Inputs:\n",
    "        data_folder (string) : Path of the data folder\n",
    "    \n",
    "    Outputs:\n",
    "        train_dataset, validation_dataset, test_dataset (Dataset) : Dataset objects for training, validation and test\n",
    "                                                                    datasets\n",
    "        test_ids (List) : List of ID's for corresponding sentences in the test dataset\n",
    "    \"\"\"\n",
    "    # Read in json data files\n",
    "    trainingSentenceFile = os.path.join(data_folder, \"train.json\")\n",
    "    validationSentenceFile = os.path.join(data_folder, \"validation.json\")\n",
    "    testSentenceFile = os.path.join(data_folder, \"test.json\")\n",
    "    \n",
    "    with open(trainingSentenceFile, 'r') as train_file:\n",
    "        trainingSentences = json.load(train_file)\n",
    "    with open(validationSentenceFile, 'r') as validation_file:\n",
    "        validationSentences = json.load(validation_file)\n",
    "    with open(testSentenceFile, 'r') as test_file:\n",
    "        testSentences = json.load(test_file)\n",
    "    \n",
    "    # Build word to id mapping\n",
    "    word_to_id = build_vocabulary(trainingSentences)\n",
    "    \n",
    "    # Create Dataset objects and list of test IDs\n",
    "    train_dataset = create_dataset(trainingSentences, word_to_id)\n",
    "    validation_dataset = create_dataset(validationSentences, word_to_id)\n",
    "    test_dataset = create_dataset(testSentences, word_to_id)\n",
    "    test_ids = [i[\"ID\"] for i in testSentences]\n",
    "    \n",
    "    # Return word to id mapping, Dataset objects and test IDs\n",
    "    return word_to_id, train_dataset, validation_dataset, test_dataset, test_ids\n",
    "    \n",
    "    \n",
    "\n",
    "model_file_path = os.path.join('models', 'fasttext_model_file_q2-2')\n",
    "word_to_id, train_dataset, valid_dataset, test_dataset, test_ids = load_question_2_2('question_2-2_data')\n",
    "model = FastText(len(word_to_id)+2, num_classes, embedding_dim=embedding_dim, learning_rate=learning_rate)\n",
    "train_fast_text(model, train_dataset, valid_dataset, test_dataset, model_file_path, batch_size=100, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def write_results(data_folder):\n",
    "    \"\"\" \n",
    "    Write prediction results to a csv file according to kaggle's format.\n",
    "    \n",
    "    Inputs:\n",
    "        data_folder (string) : Path of the folder to write results to\n",
    "    \"\"\"\n",
    "    predictions = os.path.join(data_folder, \"fasttext_model_file_q2-2predictions.csv\")\n",
    "    df = pd.read_csv(predictions, header=None, names=[\"category\"])\n",
    "    df.insert(0, \"id\", test_ids, True)\n",
    "    \n",
    "    output_file = os.path.join(data_folder, \"q2-2predictions.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "write_results(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
